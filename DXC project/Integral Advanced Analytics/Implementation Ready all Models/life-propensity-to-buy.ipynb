{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propensity To Buy Prediction\n",
    "## Data used for analysis is based on Integral Life Admin System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The SQL query in below cells will retreive data based on database credentials provide. Right now these credentials have been left empty for security reasons. Please enter relevant details before running this notebook further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aaacharya\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "# For Graphics\n",
    "import matplotlib as matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For splitting data into test and train subsets\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, GridSearchCV\n",
    "\n",
    "# For logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# For saving the model\n",
    "from sklearn.externals import joblib \n",
    "\n",
    "# For Confusion Matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "# For ROC Curve\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# For Normalization\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# For Feature Selection\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# Settings to view all columns and rows\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter ip address and port number of the system where the database resides.\n",
    "server   = '10.0.3.98'\n",
    "database = 'INT77DB_R212'\n",
    "username = 'sisensedb_user'\n",
    "password = 'Sisense12#$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "('08001', '[08001] [Microsoft][ODBC SQL Server Driver][DBNETLIB]SQL Server does not exist or access denied. (17) (SQLDriverConnect); [08001] [Microsoft][ODBC SQL Server Driver][DBNETLIB]ConnectionOpen (Connect()). (53)')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5b111b10e556>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Add appropriate driver name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcnxn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyodbc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DRIVER={SQL Server};SERVER='\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mserver\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m';DATABASE='\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m';UID='\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0musername\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m';PWD='\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mpassword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnxn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOperationalError\u001b[0m: ('08001', '[08001] [Microsoft][ODBC SQL Server Driver][DBNETLIB]SQL Server does not exist or access denied. (17) (SQLDriverConnect); [08001] [Microsoft][ODBC SQL Server Driver][DBNETLIB]ConnectionOpen (Connect()). (53)')"
     ]
    }
   ],
   "source": [
    "# Add appropriate driver name\n",
    "cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "cursor = cnxn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Query\n",
    "query = '''SELECT \n",
    "Q1.CLIENT_NUMBER,\n",
    "Q1.HAS_BOUGHT_MORE,\n",
    "SUM(Q1.NUMBER_OF_COVERAGES) AS NUMBER_OF_COVERAGES,\n",
    "Q1.FEMALE,\n",
    "Q1.MALE,\n",
    "Q1.GENDER_UNKNOWN,\n",
    "Q1.MARRIED,\n",
    "Q1.DIVORCED,\n",
    "Q1.SINGLE,\n",
    "Q1.MARITAL_STATUS_UNKNOWN,\n",
    "Q1.PROFESSION,\n",
    "Q1.CHILD,\n",
    "Q1.YOUTH,\n",
    "Q1.ADULT,\n",
    "Q1.SENIOR_CITIZEN,\n",
    "Q1.NON_SMOKER,\n",
    "Q1.SMOKER,\n",
    "Q1.UNDER_WEIGHT,\n",
    "Q1.NORMAL_WEIGHT,\n",
    "Q1.OVER_WEIGHT,\n",
    "Q1.OBESE,\n",
    "Q1.Q1_BUY,\n",
    "Q1.Q2_BUY,\n",
    "Q1.Q3_BUY,\n",
    "Q1.Q4_BUY,\n",
    "SUM(Q1.PREMIUM_PAID) AS PREMIUM_PAID\n",
    "\n",
    "FROM\n",
    "(\n",
    "SELECT DISTINCT\n",
    "CLNT.CLNTNUM AS 'CLIENT_NUMBER',\n",
    "CASE\n",
    "WHEN \n",
    "Q1.UNIQUE_PRODUCTS > 1 \n",
    "THEN \n",
    "1\n",
    "ELSE\n",
    "0\n",
    "END \n",
    "AS 'HAS_BOUGHT_MORE',\n",
    "CASE\n",
    "WHEN\n",
    "COVR.COVERS IS NULL\n",
    "THEN\n",
    "0\n",
    "ELSE\n",
    "COVR.COVERS\n",
    "END AS NUMBER_OF_COVERAGES,\n",
    "CASE \n",
    "WHEN \n",
    "CLNT.CLTSEX = 'F'\n",
    "THEN \n",
    "'1'\n",
    "ELSE\n",
    "'0'\n",
    "END AS 'FEMALE',\n",
    "CASE \n",
    "WHEN \n",
    "CLNT.CLTSEX = 'M'\n",
    "THEN \n",
    "'1'\n",
    "ELSE\n",
    "'0'\n",
    "END AS 'MALE',\n",
    "CASE \n",
    "WHEN \n",
    "CLNT.CLTSEX = ''\n",
    "THEN \n",
    "'1'\n",
    "ELSE\n",
    "'0'\n",
    "END AS 'GENDER_UNKNOWN',\n",
    "CASE \n",
    "WHEN \n",
    "CLNT.MARRYD = 'M'\n",
    "THEN \n",
    "'1'\n",
    "ELSE\n",
    "'0'\n",
    "END AS 'MARRIED',\n",
    "CASE \n",
    "WHEN \n",
    "CLNT.MARRYD = 'D'\n",
    "THEN \n",
    "'1'\n",
    "ELSE\n",
    "'0'\n",
    "END AS 'DIVORCED',\n",
    "CASE \n",
    "WHEN \n",
    "CLNT.MARRYD = 'S'\n",
    "THEN \n",
    "'1'\n",
    "ELSE\n",
    "'0'\n",
    "END AS 'SINGLE',\n",
    "CASE \n",
    "WHEN \n",
    "CLNT.MARRYD IN ('J', 'L', 'C', 'Z')\n",
    "THEN \n",
    "'1'\n",
    "ELSE\n",
    "'0'\n",
    "END AS 'MARITAL_STATUS_UNKNOWN',\n",
    "CASE\n",
    "WHEN\n",
    "LIFE.OCCUP = ''\n",
    "THEN\n",
    "'PROF_UNKNOWN'\n",
    "ELSE\n",
    "rtrim(T3644.LONGDESC)\n",
    "END  AS 'PROFESSION',\n",
    "CASE\n",
    "WHEN LIFE.ANBCCD BETWEEN 0 AND 14\n",
    "THEN \n",
    "'1'\n",
    "ELSE\n",
    "'0' \n",
    "END AS 'CHILD',\n",
    "CASE\n",
    "WHEN LIFE.ANBCCD BETWEEN 15 AND 24\n",
    "THEN \n",
    "'1'\n",
    "ELSE\n",
    "'0' \n",
    "END AS 'YOUTH',\n",
    "CASE\n",
    "WHEN LIFE.ANBCCD BETWEEN 25 AND 64\n",
    "THEN \n",
    "'1'\n",
    "ELSE\n",
    "'0' \n",
    "END AS 'ADULT',\n",
    "CASE\n",
    "WHEN LIFE.ANBCCD > 65 \n",
    "THEN \n",
    "'1'\n",
    "ELSE\n",
    "'0' \n",
    "END AS 'SENIOR_CITIZEN',\n",
    "CASE\n",
    "WHEN\n",
    "LIFE.SMOKING = 'N'\n",
    "THEN\n",
    "'1'\n",
    "ELSE\n",
    "'0'\n",
    "END AS 'NON_SMOKER',\n",
    "CASE\n",
    "WHEN\n",
    "LIFE.SMOKING <> 'N'\n",
    "THEN\n",
    "'1'\n",
    "ELSE\n",
    "'0'\n",
    "END AS 'SMOKER',\n",
    "\n",
    "CASE \n",
    "WHEN\n",
    "UNDL.BMI < 18.5\n",
    "THEN\n",
    "'1'\n",
    "ELSE\n",
    "'0'\n",
    "END AS 'UNDER_WEIGHT',\n",
    "CASE \n",
    "WHEN\n",
    "UNDL.BMI BETWEEN 18.5 AND 24.9\n",
    "THEN\n",
    "'1'\n",
    "ELSE\n",
    "'0'\n",
    "END AS 'NORMAL_WEIGHT',\n",
    "CASE \n",
    "WHEN\n",
    "UNDL.BMI BETWEEN 25 AND 29.9\n",
    "THEN\n",
    "'1'\n",
    "ELSE\n",
    "'0'\n",
    "END AS 'OVER_WEIGHT',\n",
    "CASE \n",
    "WHEN\n",
    "UNDL.BMI > 30\n",
    "THEN\n",
    "'1'\n",
    "ELSE\n",
    "'0'\n",
    "END AS 'OBESE',\n",
    "\n",
    "(COALESCE(QQ.[01],0) + COALESCE(QQ.[02],0) + COALESCE(QQ.[03],0)) AS Q1_BUY, \n",
    "(COALESCE(QQ.[04],0) + COALESCE(QQ.[05],0) + COALESCE(QQ.[06],0)) AS Q2_BUY, \n",
    "(COALESCE(QQ.[07],0) + COALESCE(QQ.[08],0) + COALESCE(QQ.[09],0)) AS Q3_BUY, \n",
    "(COALESCE(QQ.[10],0) + COALESCE(QQ.[11],0) + COALESCE(QQ.[12],0)) AS Q4_BUY,\n",
    "\n",
    "isnull(ACMV.PREMIUM_PAID,0) as PREMIUM_PAID\n",
    "\n",
    "FROM \n",
    "VM1DTA.CLNTPF CLNT\n",
    "\n",
    "INNER JOIN \n",
    "\n",
    "(SELECT  COWNNUM, COUNT( DISTINCT CNTTYPE) UNIQUE_PRODUCTS FROM VM1DTA.CHDRPF WHERE VALIDFLAG = '1' AND CHDRCOY = '2' \n",
    "GROUP BY COWNNUM\n",
    "HAVING COUNT( DISTINCT CNTTYPE) > 0\n",
    ") Q1\n",
    "\n",
    "ON CLNT.CLNTNUM = Q1.COWNNUM\n",
    "\n",
    "LEFT OUTER JOIN \n",
    "(SELECT LIFE01.CHDRNUM, LIFE01.VALIDFLAG, LIFE01.LIFE, LIFE01.JLIFE, LIFE01.LIFCNUM, LIFE01.OCCUP, LIFE01.SMOKING, LIFE01.ANBCCD\n",
    "FROM\n",
    "(select UNIQUE_NUMBER, CHDRNUM, VALIDFLAG, LIFCNUM, LIFE, JLIFE, OCCUP, SMOKING, ANBCCD  from VM1DTA.LIFEPF LIFE01 ) LIFE01 \n",
    "inner join \n",
    "(SELECT LIFCNUM, MAX(UNIQUE_NUMBER) AS MAX_UNIQUE_NUMBER FROM VM1DTA.LIFEPF GROUP BY LIFCNUM) LIFE02 \n",
    "ON LIFE01.LIFCNUM = LIFE02.LIFCNUM AND LIFE01.UNIQUE_NUMBER = LIFE02.MAX_UNIQUE_NUMBER ) LIFE\n",
    "ON LIFE.LIFCNUM = CLNT.CLNTNUM\n",
    "\n",
    "INNER JOIN \n",
    "VM1DTA.UNDLPF UNDL\n",
    "ON UNDL.CHDRNUM = LIFE.CHDRNUM AND UNDL.LIFE = LIFE.LIFE AND UNDL.JLIFE = LIFE.JLIFE\n",
    "\n",
    "LEFT  OUTER JOIN \n",
    "(select rdocnum, SUM (ACCTAMT) PREMIUM_PAID from vm1dta.acmvpf where SACSCODE='LE' AND SACSTYP IN ('LP','SP') \n",
    "and batctrcde in ('B522', 'T679', 'T642') GROUP BY RDOCNUM) ACMV\n",
    "ON ACMV.RDOCNUM  = UNDL.CHDRNUM\n",
    "\n",
    "LEFT  OUTER JOIN \n",
    "(select chdrnum, count(distinct crtable) covers from vm1dta.covrpf where VALIDFLAG='1'\n",
    "GROUP BY chdrnum) covr\n",
    "ON COVR.CHDRNUM  = UNDL.CHDRNUM\n",
    "\n",
    "INNER JOIN\n",
    "(SELECT *\n",
    "FROM\n",
    "(\n",
    "  SELECT DISTINCT COWNNUM, COUNT(DISTINCT CHDRNUM) AS COUNTING,SUBSTRING((CAST(OCCDATE AS CHAR)),5,2) AS QQ FROM VM1DTA.CHDRPF WHERE VALIDFLAG = '1' \n",
    "  GROUP BY COWNNUM, SUBSTRING((CAST(OCCDATE AS CHAR)),5,2)\n",
    ") AS SourceTable PIVOT( SUM(COUNTING) FOR [QQ] IN([01], [02],[03], [04],[05], [06],[07], [08],[09], [10],[11],\n",
    " [12])) AS PivotTable ) QQ\n",
    "\n",
    " ON QQ.COWNNUM = CLNT.CLNTNUM\n",
    "\n",
    "LEFT OUTER JOIN\n",
    "VM1DTA.DESCPF T3644\n",
    "ON T3644.DESCITEM = LIFE.OCCUP AND T3644.DESCTABL = 'T3644' AND T3644.LANGUAGE = 'E' AND T3644.DESCCOY = '9'\n",
    "\n",
    "WHERE CLNT.VALIDFLAG = '1'  AND LIFE.VALIDFLAG = '1' \n",
    ") Q1\n",
    "\n",
    "GROUP BY \n",
    "Q1.CLIENT_NUMBER,\n",
    "Q1.HAS_BOUGHT_MORE,\n",
    "Q1.FEMALE,\n",
    "Q1.MALE,\n",
    "Q1.GENDER_UNKNOWN,\n",
    "Q1.MARRIED,\n",
    "Q1.DIVORCED,\n",
    "Q1.SINGLE,\n",
    "Q1.MARITAL_STATUS_UNKNOWN,\n",
    "Q1.PROFESSION,\n",
    "Q1.CHILD,\n",
    "Q1.YOUTH,\n",
    "Q1.ADULT,\n",
    "Q1.SENIOR_CITIZEN,\n",
    "Q1.NON_SMOKER,\n",
    "Q1.SMOKER,\n",
    "Q1.UNDER_WEIGHT,\n",
    "Q1.NORMAL_WEIGHT,\n",
    "Q1.OVER_WEIGHT,\n",
    "Q1.OBESE,\n",
    "Q1.Q1_BUY,\n",
    "Q1.Q2_BUY,\n",
    "Q1.Q3_BUY,\n",
    "Q1.Q4_BUY\n",
    "\n",
    "ORDER BY Q1.CLIENT_NUMBER\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df_orig = pd.read_sql(query,cnxn)\n",
    "\n",
    "# Close the cursor\n",
    "cursor.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data sample\n",
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data shape\n",
    "df_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data attributes of the columns\n",
    "df_orig.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View distribution of 'Lapse' Instances\n",
    "df_orig['HAS_BOUGHT_MORE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any nulls\n",
    "df_orig.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study distribution of Numerical Columns\n",
    "num = [f for f in df_orig.columns if df_orig.dtypes[f] != 'object']\n",
    "nd = pd.melt(df_orig, value_vars = num)\n",
    "n1 = sns.FacetGrid (nd, col='variable', col_wrap=4, sharex=False, sharey = False)\n",
    "n1 = n1.map(sns.distplot, 'value')\n",
    "n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot count values\n",
    "df_orig.HAS_BOUGHT_MORE.value_counts()\n",
    "sns.countplot(x = 'HAS_BOUGHT_MORE', data = df_orig, palette = 'hls')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features into categorical and numerical category\n",
    "cat_vars = ['FEMALE','MALE','GENDER_UNKNOWN','MARRIED','DIVORCED',\n",
    "            'SINGLE','MARITAL_STATUS_UNKNOWN','PROFESSION','CHILD',\n",
    "            'YOUTH','ADULT','SENIOR_CITIZEN','NON_SMOKER','SMOKER',\n",
    "            'UNDER_WEIGHT','NORMAL_WEIGHT','OVER_WEIGHT','OBESE']\n",
    "\n",
    "num_vars = ['Q1_BUY', 'Q2_BUY', 'Q3_BUY','Q4_BUY', 'PREMIUM_PAID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of dataframe\n",
    "df_work = df_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to 'category' object type\n",
    "for var in cat_vars:\n",
    "    df_work[var] = df_work[var].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dummy Variables for categorical variables\n",
    "for var in cat_vars:\n",
    "    cat_list = 'var'+'_'+var\n",
    "    cat_list = pd.get_dummies(df_work[var], prefix=var)\n",
    "    dummy_data = df_work.join(cat_list)\n",
    "    df_work = dummy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data shape after dummy encoding\n",
    "df_work.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove original columns for which dummy variables have been created and ones which will not be required for prediction\n",
    "vars_remove = ['FEMALE','MALE','GENDER_UNKNOWN','MARRIED','DIVORCED','SINGLE','MARITAL_STATUS_UNKNOWN',\n",
    "               'PROFESSION','CHILD','YOUTH','ADULT','SENIOR_CITIZEN','NON_SMOKER','SMOKER', \n",
    "               'UNDER_WEIGHT','NORMAL_WEIGHT','OVER_WEIGHT','OBESE','CLIENT_NUMBER']\n",
    "\n",
    "for var in vars_remove:\n",
    "    df_work=df_work.drop(var, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical columns\n",
    "scaler = MinMaxScaler()\n",
    "df_work['Q1_BUY'] = scaler.fit_transform(df_work['Q1_BUY'].values.reshape(-1,1))\n",
    "df_work['Q2_BUY'] = scaler.fit_transform(df_work['Q2_BUY'].values.reshape(-1,1))\n",
    "df_work['Q3_BUY'] = scaler.fit_transform(df_work['Q3_BUY'].values.reshape(-1,1))\n",
    "df_work['Q4_BUY'] = scaler.fit_transform(df_work['Q4_BUY'].values.reshape(-1,1))\n",
    "df_work['PREMIUM_PAID'] = scaler.fit_transform(df_work['PREMIUM_PAID'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate dataset for feature and target columns\n",
    "x = df_work.drop('HAS_BOUGHT_MORE', axis=1)\n",
    "y = df_work['HAS_BOUGHT_MORE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestfeatures=SelectKBest(score_func=chi2, k=10)\n",
    "fit=bestfeatures.fit(x,y)\n",
    "dfscores=pd.DataFrame(fit.scores_)\n",
    "dfcolumns=pd.DataFrame(x.columns)\n",
    "#concatenate two dataframes\n",
    "featureScores=pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns=['Specs','Score'] #naming the dataframe columns\n",
    "print(featureScores.nlargest(10,'Score')) #print 10 best features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top features\n",
    "top_features = ['Q1_BUY', 'PROFESSION_Analyst', 'FEMALE_1','MALE_0','PROFESSION_Architect','Q4_BUY',\n",
    "                'PROFESSION_Applications Consultant','PROFESSION_Juvenile Insured',\n",
    "                'PROFESSION_Actor/Actress', 'PROFESSION_Doctor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataset with only top features\n",
    "x01=x[top_features]\n",
    "y01=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Test/Train Split\n",
    "x01_train, x01_test, y01_train, y01_test=train_test_split(x01, y01, test_size=0.3,random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To handle class imbalance performing SMOTE\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "SMO = SMOTE(random_state=11)\n",
    "\n",
    "#X4_train, X4_test, Y4_train, Y4_test = train_test_split(X_train,Y_train, test_size=0.20)\n",
    "x01_train_smo,y01_train_smo = SMO.fit_sample(x01_train, y01_train)\n",
    "columns = x01_train.columns\n",
    "\n",
    "x01_train_smo = pd.DataFrame(data = x01_train_smo, columns = columns )\n",
    "y01_train_smo = pd.DataFrame(data = y01_train_smo, columns = ['HAS_BOUGHT_MORE'])\n",
    "\n",
    "# we can Check the numbers of our data\n",
    "print(\"length of oversampled data is \",len(x01_train_smo))\n",
    "print(\"Number of clients with multiple policies in oversampled data\",len(y01_train_smo[y01_train_smo['HAS_BOUGHT_MORE']== 0 ]))\n",
    "print(\"Number of clients with a single policy in oversampled data\",len(y01_train_smo[y01_train_smo['HAS_BOUGHT_MORE']== 1 ]))\n",
    "print(\"Proportion of clients with a single policy in oversampled data is \",len(y01_train_smo[y01_train_smo['HAS_BOUGHT_MORE']== 0])/len(x01_train_smo))\n",
    "print(\"Proportion of clients with multiple policies in oversampled data is \",len(y01_train_smo[y01_train_smo['HAS_BOUGHT_MORE']== 1 ])/len(x01_train_smo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with StratifiedKFold with set of weights provided.  Also use Gridsearch to obtain optimal parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grid\n",
    "C = np.logspace(0,5,10)\n",
    "param_grid = dict( C=C)\n",
    "\n",
    "# Model\n",
    "logreg=LogisticRegression(solver='lbfgs',max_iter=2000)\n",
    "\n",
    "# Define Evaluation Procedure\n",
    "cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=11)\n",
    "\n",
    "# Define Grid Search\n",
    "grid=GridSearchCV(estimator=logreg, param_grid=param_grid, n_jobs=1, cv=cv, scoring='roc_auc')\n",
    "\n",
    "# Execute Grid Search\n",
    "grid_result=grid.fit(x01_train_smo, (y01_train_smo.values.ravel()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best model parameters\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the best model\n",
    "bestlogreg=grid_result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on train data\n",
    "bestlogreg.fit(x01_train, y01_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Predictions\n",
    "y01_pred=bestlogreg.predict(x01_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Confusion Matrix to compare results against actuals\n",
    "cnf_matrix = metrics.confusion_matrix(y01_test, y01_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Accuracy Scores\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y01_test, y01_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y01_test, y01_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y01_test, y01_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print ROC AUC Curve\n",
    "\n",
    "logit_roc_auc = roc_auc_score(y01_test, bestlogreg.predict(x01_test))\n",
    "fpr, tpr, thresholds = roc_curve(y01_test, bestlogreg.predict_proba(x01_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model as PMML File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model as a pmml and use to make predictions in a file \n",
    "\n",
    "joblib.dump(bestlogreg, 'LifePropensityModel.pmml') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model on entire dataset and save the output in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the original data set\n",
    "df_test = df_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features into categorical and numerical category\n",
    "cat_vars = ['FEMALE','MALE','GENDER_UNKNOWN','MARRIED','DIVORCED',\n",
    "            'SINGLE','MARITAL_STATUS_UNKNOWN','PROFESSION','CHILD',\n",
    "            'YOUTH','ADULT','SENIOR_CITIZEN','NON_SMOKER','SMOKER',\n",
    "            'UNDER_WEIGHT','NORMAL_WEIGHT','OVER_WEIGHT','OBESE']\n",
    "\n",
    "num_vars = ['Q1_BUY', 'Q2_BUY', 'Q3_BUY','Q4_BUY', 'NUMBER_OF_CLAIMS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to 'category' object type\n",
    "for var in cat_vars:\n",
    "    df_test[var] = df_test[var].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dummy Variables for categorical variables\n",
    "for var in cat_vars:\n",
    "    cat_list = 'var'+'_'+var\n",
    "    cat_list = pd.get_dummies(df_test[var], prefix=var)\n",
    "    dummy_data = df_test.join(cat_list)\n",
    "    df_test = dummy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove original columns for which dummy variables have been created and ones which will not be required for prediction\n",
    "vars_remove = ['FEMALE','MALE','GENDER_UNKNOWN','MARRIED','DIVORCED','SINGLE','MARITAL_STATUS_UNKNOWN',\n",
    "               'PROFESSION','CHILD','YOUTH','ADULT','SENIOR_CITIZEN','NON_SMOKER','SMOKER', \n",
    "               'UNDER_WEIGHT','NORMAL_WEIGHT','OVER_WEIGHT','OBESE','CLIENT_NUMBER']\n",
    "\n",
    "for var in vars_remove:\n",
    "    df_test=df_test.drop(var, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top features\n",
    "top_features = ['Q1_BUY', 'PROFESSION_Analyst', 'FEMALE_1','MALE_0','PROFESSION_Architect','Q4_BUY',\n",
    "                'PROFESSION_Applications Consultant','PROFESSION_Juvenile Insured',\n",
    "                'PROFESSION_Actor/Actress', 'PROFESSION_Doctor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain only top features in the datframe\n",
    "df_test=df_test[top_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "prediction_class = bestlogreg.predict(df_test)\n",
    "prediction_prob = bestlogreg.predict_proba(df_test)\n",
    "\n",
    "# Get Individual Probabilities\n",
    "notbuy, buy = (prediction_prob).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save original dataframe in a separate dataframe\n",
    "df_results = df_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove target column\n",
    "df_results=df_results.drop('HAS_BOUGHT_MORE',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace target column with predicted value of '0' or '1' along with respective probablities\n",
    "df_results['notbuy'] = notbuy\n",
    "df_results['buy'] = buy\n",
    "df_results['propensity'] = prediction_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Though care has been taken in testing the query output, still double check for duplicate rows for a policy in the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv('EX_SOURCE_CUSTOMER_PROPENSITY_LIFE.csv', index = None, header=True) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
