{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aaacharya\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "# Get Current time\n",
    "import datetime \n",
    "\n",
    "# For Graphics\n",
    "import matplotlib as matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For splitting data into test and train subsets\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, GridSearchCV\n",
    "\n",
    "# For logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# For saving the model\n",
    "from sklearn.externals import joblib \n",
    "\n",
    "# For Confusion Matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "# For ROC Curve\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# For Normalization\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# For Feature Selection\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# Settings to view all columns and rows\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "\n",
    "#to ignore the warning messages from being printed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "# Explicitly setting data type for columns which are string data type  \n",
    "\n",
    "df_orig = pd.read_csv('agent_churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of dataframe\n",
    "df_work = df_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features into categorical and numerical category\n",
    "cat_vars = ['Commission Class','Pay Method','Pay Frequency','Agent Type','Agent Branch Code',\n",
    "            'Agent Reporting Level','Agent Gender']\n",
    "\n",
    "num_vars = ['Number_of_Policies', 'Number_of_Claims', 'Total_Claim_Amount','Total_Payment_Amount','Total_Commission_Amount']\n",
    "\n",
    "date_vars = ['Date_of_Appointment', 'Date_of_Termination', 'Agent Date of Birth' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns\n",
    "\n",
    "for var in date_vars:\n",
    "    #df_work[var] = df_work[var].astype('category')\n",
    "    #df_work[var] = datetime.datetime.strptime(var, '%Y-%m-%d')\n",
    "    df_work[var]=pd.to_datetime(df_work[var].astype(str), format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to 'category' object type\n",
    "for var in cat_vars:\n",
    "    df_work[var] = df_work[var].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dummy Variables for categorical variables\n",
    "for var in cat_vars:\n",
    "    cat_list = 'var'+'_'+var\n",
    "    cat_list = pd.get_dummies(df_work[var], prefix=var)\n",
    "    dummy_data = df_work.join(cat_list)\n",
    "    df_work = dummy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove original columns for which dummy variables have been created\n",
    "vars_remove = ['Commission Class','Pay Method','Pay Frequency','Agent Type','Agent Branch Code',\n",
    "            'Agent Reporting Level','Agent Gender']\n",
    "\n",
    "for var in vars_remove:\n",
    "    df_work=df_work.drop(var, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical columns\n",
    "scaler = MinMaxScaler()\n",
    "df_work['Number_of_Policies'] = scaler.fit_transform(df_work['Number_of_Policies'].values.reshape(-1,1))\n",
    "df_work['Number_of_Claims'] = scaler.fit_transform(df_work['Number_of_Claims'].values.reshape(-1,1))\n",
    "df_work['Total_Claim_Amount'] = scaler.fit_transform(df_work['Total_Claim_Amount'].values.reshape(-1,1))\n",
    "df_work['Total_Payment_Amount'] = scaler.fit_transform(df_work['Total_Payment_Amount'].values.reshape(-1,1))\n",
    "df_work['Total_Commission_Amount'] = scaler.fit_transform(df_work['Total_Commission_Amount'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current date\n",
    "current_time = datetime.datetime.now() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'Age' on the basis of date of birth of agent\n",
    "df_work['age'] = current_time - df_work['Agent Date of Birth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column named 'service period' on the basis of difference between Agent Date of Registration and Termination.\n",
    "# In case date of Termination is null we will replace it with current date and compute\n",
    "\n",
    "service_period=[]\n",
    "\n",
    "for Date_of_Termination,Date_of_Appointment in zip(df_work.iloc[:,2], df_work.iloc[:,1]):\n",
    "    if Date_of_Termination is pd.NaT:\n",
    "        service_period.append(abs(current_time-Date_of_Appointment))\n",
    "    else:\n",
    "        service_period.append(abs(Date_of_Termination-Date_of_Appointment))\n",
    "    #print(Date_of_Termination)\n",
    "    #print(Date_of_Appointment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the new column to dataframe\n",
    "df_work['Service_Period']=service_period\n",
    "\n",
    "# Remove the orginal columns\n",
    "df_work=df_work.drop('Date_of_Appointment', axis=1)\n",
    "df_work=df_work.drop('Date_of_Termination', axis=1)\n",
    "df_work=df_work.drop('Agent Date of Birth', axis=1)\n",
    "\n",
    "# Remove Agent Number since it is not critical for prediction\n",
    "df_work=df_work.drop('Agent Number', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the new columns \n",
    "scaler = MinMaxScaler()\n",
    "df_work['age'] = scaler.fit_transform(df_work['age'].values.reshape(-1,1))\n",
    "df_work['Service_Period'] = scaler.fit_transform(df_work['Service_Period'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate dataset for feature and target columns\n",
    "x = df_work.drop('Churn', axis=1)\n",
    "y = df_work['Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2541, 32), (1090, 32), (2541,), (1090,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform Test/Train Split\n",
    "#x_train, x_test, y_train, y_test=train_test_split(x, y, test_size=0.3,random_state=11)\n",
    "#x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of oversampled data is  7238\n",
      "Number of Churned agents in oversampled data 3619\n",
      "Number of Non Churned agents in oversampled data 3619\n",
      "Proportion of Non Churned agents in oversampled data is  0.5\n",
      "Proportion of Churned Agents in oversampled data is  0.5\n"
     ]
    }
   ],
   "source": [
    "# To handle class imbalance performing SMOTE\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "SMO = SMOTE(random_state=11)\n",
    "\n",
    "#X4_train, X4_test, Y4_train, Y4_test = train_test_split(X_train,Y_train, test_size=0.20)\n",
    "x_smo,y_smo = SMO.fit_sample(x, y)\n",
    "columns = x_train.columns\n",
    "\n",
    "x_smo = pd.DataFrame(data = x_smo, columns = columns )\n",
    "y_smo = pd.DataFrame(data = y_smo, columns = ['Churn'])\n",
    "\n",
    "# we can Check the numbers of our data\n",
    "print(\"length of oversampled data is \",len(x_smo))\n",
    "print(\"Number of Churned agents in oversampled data\",len(y_smo[y_smo['Churn']== 0 ]))\n",
    "print(\"Number of Non Churned agents in oversampled data\",len(y_smo[y_smo['Churn']== 1 ]))\n",
    "print(\"Proportion of Non Churned agents in oversampled data is \",len(y_smo[y_smo['Churn']== 0])/len(x_smo))\n",
    "print(\"Proportion of Churned Agents in oversampled data is \",len(y_smo[y_smo['Churn']== 1 ])/len(x_smo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Specs       Score\n",
      "14            Agent Type_AM  421.830701\n",
      "0        Number_of_Policies  386.071018\n",
      "1          Number_of_Claims  280.271562\n",
      "18            Agent Type_BR  251.136585\n",
      "27           Agent Gender_   201.000000\n",
      "4   Total_Commission_Amount  199.688720\n",
      "5        Commission Class_1  187.000000\n",
      "6        Commission Class_2  160.370031\n",
      "28           Agent Gender_F  143.000000\n",
      "2        Total_Claim_Amount  137.999453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14              Agent Type_AM\n",
       "0          Number_of_Policies\n",
       "1            Number_of_Claims\n",
       "18              Agent Type_BR\n",
       "27             Agent Gender_ \n",
       "4     Total_Commission_Amount\n",
       "5          Commission Class_1\n",
       "6          Commission Class_2\n",
       "28             Agent Gender_F\n",
       "2          Total_Claim_Amount\n",
       "Name: Specs, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestfeatures=SelectKBest(score_func=chi2, k=10)\n",
    "fit=bestfeatures.fit(x_smo,y_smo)\n",
    "dfscores=pd.DataFrame(fit.scores_)\n",
    "dfcolumns=pd.DataFrame(x_smo.columns)\n",
    "#concatenate two dataframes\n",
    "featureScores=pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns=['Specs','Score'] #naming the dataframe columns\n",
    "print(featureScores.nlargest(10,'Score')) #print 10 best features\n",
    "top10 = featureScores.nlargest(10,'Score')  #select the 10 best features with scores\n",
    "top_features = top10['Specs']               #10 best feature cols \n",
    "top_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataset with only top features\n",
    "x01=x[top_features]\n",
    "y01=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5066, 32), (2172, 32), (5066, 1), (2172, 1))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform Test/Train Split\n",
    "x01_train, x01_test, y01_train, y01_test=train_test_split(x_smo, y_smo, test_size=0.3,random_state=11)\n",
    "x01_train.shape, x01_test.shape, y01_train.shape, y01_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x01_train_smo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-b02802ecc9a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Execute Grid Search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mgrid_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx01_train_smo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my01_train_smo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x01_train_smo' is not defined"
     ]
    }
   ],
   "source": [
    "# Define grid\n",
    "C = np.logspace(0,5,10)\n",
    "param_grid = dict( C=C)\n",
    "\n",
    "# Model\n",
    "logreg=LogisticRegression(solver='lbfgs',max_iter=1000)\n",
    "\n",
    "# Define Evaluation Procedure\n",
    "cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=11)\n",
    "\n",
    "# Define Grid Search\n",
    "grid=GridSearchCV(estimator=logreg, param_grid=param_grid, n_jobs=1, cv=cv, scoring='roc_auc')\n",
    "\n",
    "# Execute Grid Search\n",
    "grid_result=grid.fit(x01_train, (y01_train_smo.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
