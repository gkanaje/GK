{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Churn Prediction\n",
    "## Data used for analysis is based on Integral Life Admin System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The SQL query in below cells will retreive data based on database credentials provide. Right now these credentials have been left empty for security reasons. Please enter relevant details before running this notebook further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aaacharya\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "# Get Current time\n",
    "import datetime \n",
    "\n",
    "# For Graphics\n",
    "import matplotlib as matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For splitting data into test and train subsets\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, GridSearchCV\n",
    "\n",
    "# For logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# For saving the model\n",
    "from sklearn.externals import joblib \n",
    "\n",
    "# For Confusion Matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "# For ROC Curve\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# For Normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# For Feature Selection\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Settings to view all columns and rows\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter ip address and port number of the system where the database(20.4 regression) resides.\n",
    "server = '10.0.3.172'\n",
    "database = 'INT77DB_204' \n",
    "username = 'sisensedb_user'\n",
    "password = 'Sisense12#$' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "('08001', '[08001] [Microsoft][ODBC SQL Server Driver][DBNETLIB]SQL Server does not exist or access denied. (17) (SQLDriverConnect); [08001] [Microsoft][ODBC SQL Server Driver][DBNETLIB]ConnectionOpen (Connect()). (53)')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5b111b10e556>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Add appropriate driver name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcnxn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyodbc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DRIVER={SQL Server};SERVER='\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mserver\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m';DATABASE='\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m';UID='\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0musername\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m';PWD='\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mpassword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnxn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOperationalError\u001b[0m: ('08001', '[08001] [Microsoft][ODBC SQL Server Driver][DBNETLIB]SQL Server does not exist or access denied. (17) (SQLDriverConnect); [08001] [Microsoft][ODBC SQL Server Driver][DBNETLIB]ConnectionOpen (Connect()). (53)')"
     ]
    }
   ],
   "source": [
    "# Add appropriate driver name\n",
    "cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "cursor = cnxn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Query\n",
    "query = '''\n",
    "select \n",
    "q1.agntnum\tas 'Agent Number',\n",
    "q1.dteapp\tas 'Date_of_Appointment',\n",
    "case\n",
    "when\n",
    "q1.dtetrm = '99999999'\n",
    "then \n",
    "''\n",
    "else\n",
    "cast(q1.dtetrm as varchar)\n",
    "end\t\t\tas 'Date_of_Termination',\n",
    "q1.agcls\tas 'Commission Class',\n",
    "q1.paymth\tas 'Pay Method',\n",
    "q1.payfrq\tas 'Pay Frequency',\n",
    "q2.agtype\tas 'Agent Type',\n",
    "q2.agntbr   as 'Agent Branch Code',\n",
    "q2.replvl\tas 'Agent Reporting Level',\n",
    "case\n",
    "when \n",
    "q3.cltdob = '99999999'\n",
    "then \n",
    "''\n",
    "else\n",
    "cast (q3.cltdob as varchar)\n",
    "end\t\t\tas 'Agent_Date_of_Birth',\n",
    "q3.cltsex\tas 'Agent Gender',\n",
    "isnull(q4.policies,0)  as 'Number_of_Policies',\n",
    "isnull(q7.claims,0)  as 'Number_of_Claims',\n",
    "isnull(q6.claim_amount,0)  as 'Total_Claim_Amount',\n",
    "isnull(q5.payments,0)  as 'Total_Payment_Amount',\n",
    "isnull(q8.commission,0)  as 'Total_Commission_Amount',\n",
    "case\n",
    "when\n",
    "q1.dtetrm = 99999999\n",
    "then \n",
    "0\n",
    "else\n",
    "1\n",
    "end\n",
    "as 'Churn'\n",
    "\n",
    "from \n",
    "(select agntnum, dteapp, dtetrm, agcls, paymth, payfrq from vm1dta.aglfpf where agntcoy='2') q1\n",
    "inner join \n",
    "(select agntnum, clntnum, agtype, agntbr, replvl from vm1dta.agntpf where agntcoy='2') q2\n",
    "on q1.agntnum = q2.agntnum\n",
    "inner join \n",
    "(select * from vm1dta.clntpf where validflag='1') q3\n",
    "on q3.clntnum = q2.clntnum\n",
    "left outer join\n",
    "(select agntnum, count(distinct chdrnum) as policies from vm1dta.chdrpf where chdrcoy = '2' group by agntnum )q4\n",
    "on q4.agntnum = q1.agntnum \n",
    "left outer join\n",
    "(\n",
    "select \n",
    "chdr.agntnum, sum(regp.payment) as payments\n",
    "from \n",
    "(select distinct agntnum, chdrnum from vm1dta.chdrpf where chdrcoy = '2') chdr \n",
    "inner join\n",
    "(select distinct chdrnum, sum(totamnt) as payment from vm1dta.regppf group by chdrnum) regp\n",
    "on regp.chdrnum = chdr.chdrnum\n",
    "group by chdr.agntnum\n",
    ")q5\n",
    "on q5.agntnum = q1.agntnum \n",
    "left outer join\n",
    "(\n",
    "select \n",
    "chdr.agntnum, sum(clmd.amount) as claim_amount\n",
    "from \n",
    "(select distinct agntnum, chdrnum from vm1dta.chdrpf where chdrcoy = '2') chdr \n",
    "inner join\n",
    "(select sum(actvalue) as amount, chdrnum from vm1dta.clmdpf group by chdrnum ) clmd\n",
    "on clmd.chdrnum = chdr.chdrnum\n",
    "group by chdr.agntnum\n",
    ")q6\n",
    "on q6.agntnum = q1.agntnum \n",
    "left outer join \n",
    "(\n",
    "select \n",
    "chdr.agntnum, count(clmd.chdrnum) as claims\n",
    "from \n",
    "(select distinct agntnum, chdrnum from vm1dta.chdrpf where chdrcoy = '2') chdr \n",
    "inner join\n",
    "(select sum(actvalue) as amount, chdrnum from vm1dta.clmdpf group by chdrnum ) clmd\n",
    "on clmd.chdrnum = chdr.chdrnum\n",
    "group by chdr.agntnum\n",
    ")q7\n",
    "on q7.agntnum = q1.agntnum \n",
    "left outer join \n",
    "(select rldgacct, sum(acctamt) as commission from vm1dta.acmvpf where rdoccoy = '2' and SACSCODE='LA'  AND SACSTYP IN ('IC','RC','SG')\n",
    "group by rldgacct)q8\n",
    "on q8.rldgacct = q1.agntnum\n",
    "\n",
    "order by q1.agntnum\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df_orig = pd.read_sql(query,cnxn)\n",
    "\n",
    "# Close the cursor\n",
    "cursor.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data sample\n",
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data shape\n",
    "df_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data attributes of the columns\n",
    "df_orig.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View distribution of 'Churn' Instances\n",
    "df_orig['Churn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any nulls\n",
    "df_orig.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot count values\n",
    "df_orig.Churn.value_counts()\n",
    "sns.countplot(x = 'Churn', data = df_orig, palette = 'hls')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of dataframe\n",
    "df_work = df_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features into categorical, numerical and category\n",
    "cat_vars = ['Commission Class','Pay Method','Pay Frequency','Agent Type','Agent Branch Code',\n",
    "            'Agent Reporting Level','Agent Gender']\n",
    "\n",
    "num_vars = ['Number_of_Policies', 'Number_of_Claims', 'Total_Claim_Amount','Total_Payment_Amount','Total_Commission_Amount']\n",
    "\n",
    "date_vars = ['Date_of_Appointment', 'Date_of_Termination', 'Agent_Date_of_Birth' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns\n",
    "date_vars = ['Date_of_Appointment','Date_of_Termination','Agent_Date_of_Birth' ]\n",
    "for var in date_vars:\n",
    "    df_work[var]=pd.to_datetime(df_work[var].astype(str), format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to 'category' object type\n",
    "for var in cat_vars:\n",
    "    df_work[var] = df_work[var].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dummy Variables for categorical variables\n",
    "for var in cat_vars:\n",
    "    cat_list = 'var'+'_'+var\n",
    "    cat_list = pd.get_dummies(df_work[var], prefix=var)\n",
    "    dummy_data = df_work.join(cat_list)\n",
    "    df_work = dummy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data shape after dummy encoding\n",
    "df_work.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove original columns for which dummy variables have been created\n",
    "vars_remove = ['Commission Class','Pay Method','Pay Frequency','Agent Type','Agent Branch Code',\n",
    "            'Agent Reporting Level','Agent Gender']\n",
    "\n",
    "for var in vars_remove:\n",
    "    df_work=df_work.drop(var, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical columns\n",
    "scaler = MinMaxScaler()\n",
    "df_work['Number_of_Policies'] = scaler.fit_transform(df_work['Number_of_Policies'].values.reshape(-1,1))\n",
    "df_work['Number_of_Claims'] = scaler.fit_transform(df_work['Number_of_Claims'].values.reshape(-1,1))\n",
    "df_work['Total_Claim_Amount'] = scaler.fit_transform(df_work['Total_Claim_Amount'].values.reshape(-1,1))\n",
    "df_work['Total_Payment_Amount'] = scaler.fit_transform(df_work['Total_Payment_Amount'].values.reshape(-1,1))\n",
    "df_work['Total_Commission_Amount'] = scaler.fit_transform(df_work['Total_Commission_Amount'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current date\n",
    "current_time = datetime.datetime.now() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'Age' on the basis of date of birth of agent\n",
    "df_work['age'] = current_time - df_work['Agent_Date_of_Birth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column named 'service period' on the basis of difference between Agent Date of Registration and Termination.\n",
    "# In case date of Termination is null we will replace it with current date and compute\n",
    "\n",
    "service_period=[]\n",
    "\n",
    "for Date_of_Termination,Date_of_Appointment in zip(df_work.iloc[:,2], df_work.iloc[:,1]):\n",
    "    if Date_of_Termination is pd.NaT:\n",
    "        service_period.append(abs(current_time-Date_of_Appointment))\n",
    "    else:\n",
    "        service_period.append(abs(Date_of_Termination-Date_of_Appointment))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the new column to dataframe\n",
    "df_work['Service_Period']=service_period\n",
    "\n",
    "# Remove the orginal columns\n",
    "df_work=df_work.drop('Date_of_Appointment', axis=1)\n",
    "df_work=df_work.drop('Date_of_Termination', axis=1)\n",
    "df_work=df_work.drop('Agent_Date_of_Birth', axis=1)\n",
    "\n",
    "# Remove Agent Number since it is not critical for prediction\n",
    "df_work=df_work.drop('Agent Number', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check contents once\n",
    "df_work.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the new columns \n",
    "scaler = MinMaxScaler()\n",
    "df_work['age'] = scaler.fit_transform(df_work['age'].values.reshape(-1,1))\n",
    "df_work['Service_Period'] = scaler.fit_transform(df_work['Service_Period'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate dataset for feature and target columns\n",
    "X = df_work.drop('Churn', axis=1)\n",
    "y = df_work['Churn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing SMOTE operation to handle class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Test/Train Split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3,random_state=11)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To handle class imbalance performing SMOTE\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "SMO = SMOTE(random_state=11)\n",
    "\n",
    "X_train_smo, y_train_smo = SMO.fit_sample(X_train, y_train)\n",
    "columns = X_train.columns\n",
    "\n",
    "X_train_smo = pd.DataFrame(data = X_train_smo, columns = columns )\n",
    "y_train_smo = pd.DataFrame(data = y_train_smo, columns = ['Churn'])\n",
    "\n",
    "# we can Check the numbers of our data\n",
    "print(\"length of oversampled data is \",len(X_train_smo))\n",
    "print(\"Number of Non Churned agents in oversampled data\",len(y_train_smo[y_train_smo['Churn']== 0 ]))\n",
    "print(\"Number of Churned agents in oversampled data\",len(y_train_smo[y_train_smo['Churn']== 1 ]))\n",
    "print(\"Proportion of Non Churned agents in oversampled data is \",len(y_train_smo[y_train_smo['Churn']== 0])/len(X_train_smo))\n",
    "print(\"Proportion of Churned Agents in oversampled data is \",len(y_train_smo[y_train_smo['Churn']== 1 ])/len(X_train_smo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPOT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import tpot classifier from tpot and initialize it to run for max generation = 5 OR max time = 60 mins\n",
    "from tpot import TPOTClassifier\n",
    "tpot = TPOTClassifier(generations=5, verbosity=2, max_time_mins=60, n_jobs = -1)\n",
    "tpot.fit(X_train_smo, y_train_smo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline\n",
    "TPOT_Fit_pipeline=tpot.fitted_pipeline_\n",
    "TPOT_Fit_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print score\n",
    "print(tpot.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Predictions\n",
    "y_pred=TPOT_Fit_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Confusion Matrix to compare results against actuals\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Accuracy Scores\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print ROC AUC Curve\n",
    "\n",
    "logit_roc_auc = roc_auc_score(y_test, TPOT_Fit_pipeline.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, TPOT_Fit_pipeline.predict_proba(X_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='TPOT Classifier (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model on entire dataset and save the output in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the original data set\n",
    "df_test = df_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features into categorical, numerical and category\n",
    "cat_vars = ['Commission Class','Pay Method','Pay Frequency','Agent Type','Agent Branch Code',\n",
    "            'Agent Reporting Level','Agent Gender']\n",
    "\n",
    "num_vars = ['Number_of_Policies', 'Number_of_Claims', 'Total_Claim_Amount','Total_Payment_Amount','Total_Commission_Amount']\n",
    "\n",
    "date_vars = ['Date_of_Appointment', 'Date_of_Termination', 'Agent_Date_of_Birth' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns\n",
    "date_vars = ['Date_of_Appointment','Date_of_Termination','Agent_Date_of_Birth' ]\n",
    "for var in date_vars:\n",
    "    df_test[var]=pd.to_datetime(df_test[var].astype(str), format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to 'category' object type\n",
    "for var in cat_vars:\n",
    "    df_test[var] = df_test[var].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dummy Variables for categorical variables\n",
    "for var in cat_vars:\n",
    "    cat_list = 'var'+'_'+var\n",
    "    cat_list = pd.get_dummies(df_test[var], prefix=var)\n",
    "    dummy_data = df_test.join(cat_list)\n",
    "    df_test = dummy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data shape after dummy encoding\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove original columns for which dummy variables have been created and also the taget value 'Churn'\n",
    "vars_remove = ['Commission Class','Pay Method','Pay Frequency','Agent Type','Agent Branch Code',\n",
    "            'Agent Reporting Level','Agent Gender', 'Churn']\n",
    "\n",
    "for var in vars_remove:\n",
    "    df_test=df_test.drop(var, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current date\n",
    "current_time = datetime.datetime.now() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'Age' on the basis of date of birth of agent\n",
    "df_test['age'] = current_time - df_test['Agent_Date_of_Birth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column named 'service period' on the basis of difference between Agent Date of Registration and Termination.\n",
    "# In case date of Termination is null we will replace it with current date and compute\n",
    "\n",
    "service_period=[]\n",
    "\n",
    "for Date_of_Termination,Date_of_Appointment in zip(df_test.iloc[:,2], df_test.iloc[:,1]):\n",
    "    if Date_of_Termination is pd.NaT:\n",
    "        service_period.append(abs(current_time-Date_of_Appointment))\n",
    "    else:\n",
    "        service_period.append(abs(Date_of_Termination-Date_of_Appointment))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the new column to dataframe\n",
    "df_test['Service_Period']=service_period\n",
    "\n",
    "# Remove the orginal columns\n",
    "df_test=df_test.drop('Date_of_Appointment', axis=1)\n",
    "df_test=df_test.drop('Date_of_Termination', axis=1)\n",
    "df_test=df_test.drop('Agent_Date_of_Birth', axis=1)\n",
    "\n",
    "# Remove Agent Number since it is not critical for prediction\n",
    "df_test=df_test.drop('Agent Number', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the new columns \n",
    "scaler = MinMaxScaler()\n",
    "df_test['age'] = scaler.fit_transform(df_test['age'].values.reshape(-1,1))\n",
    "df_test['Service_Period'] = scaler.fit_transform(df_test['Service_Period'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "prediction_class = TPOT_Fit_pipeline.predict(df_test)\n",
    "prediction_prob = TPOT_Fit_pipeline.predict_proba(df_test)\n",
    "\n",
    "# Get Individual Probabilities\n",
    "not_churn, churn = (prediction_prob).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save original dataframe in a separate dataframe\n",
    "df_results = df_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove target column\n",
    "df_results=df_results.drop('Churn',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace target column with predicted value of '0' or '1' along with respective probablities\n",
    "df_results['notchurn'] = not_churn\n",
    "df_results['churn'] = churn\n",
    "df_results['churn_prediction_class'] = prediction_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Though care has been taken in testing the query output, still double check for duplicate rows for an agent in the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv('EX_SOURCE_AGENT_CHURN_LIFE.csv', index = None, header=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model as PMML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(TPOT_Fit_pipeline, 'life_agent_churn.pmml') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
